{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f2be842-80e1-4c95-b6b2-5724545551e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37db485d-fcec-4c20-943e-239a74a70cf4",
   "metadata": {},
   "source": [
    "# Homomorphic Encrypted LeNet-1\n",
    "This notebook will show a very practical example of running the famous LeNet-1 DL model directly on encrypted data.\n",
    "\n",
    "![scheme](HE_processing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b6b3e7-d428-48e4-b9db-3609ceb94b25",
   "metadata": {},
   "source": [
    "## Homomorphic encryption operations\n",
    "First of all, we will look at Pyfhel, a Python library which wraps SEAL, one of the most used frameworks for HE.\n",
    "Pyfhel supports the BFV scheme, so, it is the one that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c74dbc3-b890-4a87-9e8e-0094b9fc17b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ckks Pyfhel obj at 0x7fd6121fe710, [pk:Y, sk:Y, rtk:-, rlk:Y, contx(n=8192, t=0, sec=128, qi=[31, 26, 26, 26, 26, 26, 26, 31], scale=67108864.0, )]>\n"
     ]
    }
   ],
   "source": [
    "from Pyfhel import Pyfhel, PyPtxt, PyCtxt\n",
    "\n",
    "n_mults = 6\n",
    "scale_power = 26\n",
    "\n",
    "HE = Pyfhel()\n",
    "HE.contextGen(scheme='ckks', n=8192, scale=2**26, qi_sizes=[31]+ [scale_power]*n_mults +[31])\n",
    "HE.keyGen()\n",
    "HE.relinKeyGen()\n",
    "\n",
    "print(HE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9f9b90a-f443-4a6b-9fbb-b1fc91c2565d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected sum: [125.02820745 127.25717263], decrypted sum: 125.02819537224904\n",
      "Expected sub: [129.28613781 127.05717263], decrypted sum: 129.2861256766009\n",
      "Expected mul: [-270.71319317   12.71571726], decrypted sum: -270.713163606141\n"
     ]
    }
   ],
   "source": [
    "a = np.array([127.15717263])\n",
    "b = -2.128965182, 0.1\n",
    "ctxt1 = HE.encrypt(a)\n",
    "ctxt2 = HE.encode(b)\n",
    "\n",
    "ctxtSum = ctxt1 + ctxt2\n",
    "ctxtSub = ctxt1 - ctxt2\n",
    "ctxtMul = ctxt1 * ctxt2\n",
    "\n",
    "resSum = HE.decrypt(ctxtSum)\n",
    "resSub = HE.decrypt(ctxtSub) \n",
    "resMul = HE.decrypt(ctxtMul)\n",
    "\n",
    "print(f\"Expected sum: {a+b}, decrypted sum: {resSum[0]}\")\n",
    "print(f\"Expected sub: {a-b}, decrypted sum: {resSub[0]}\")\n",
    "print(f\"Expected mul: {a*b}, decrypted sum: {resMul[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1180b17e-7644-4ce8-99a6-c1906758e2eb",
   "metadata": {},
   "source": [
    "## LeNet-1\n",
    "The LeNet-1 is a small CNN developed by LeCun et al. It is composed of 5 layers: a convolutional layer with 4 kernels of size 5x5 and tanh activation, an average pooling layer with kernel of size 2, another convolutional layer with 16 kernels of size 5x5 and tanh activation, another average pooling layer with kernel of size 2, and a fully connected layers with size 192x10. \n",
    "\n",
    "The highest value in the output tensor corresponds to the label LeNet-1 associated to the input image. \n",
    "\n",
    "For this tutorial we will use the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fcf92dc-4929-4adc-b97e-2a1bf946bfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "test_set = torchvision.datasets.MNIST(\n",
    "    root = './data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=50,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025dfbc9-38d0-41a7-a140-9606bfeff53f",
   "metadata": {},
   "source": [
    "## Approximating\n",
    "As we know, there are some operations that cannot be performed homomorphically on encrypted values. Most notably, these operations are division and comparison. It is possible to perform only linear functions.\n",
    "\n",
    "Consequently, in the LeNet-1 scheme we used, we can not use `tanh()`. This is because we cannot apply its non-linearities.\n",
    "\n",
    "\n",
    "One of the most common approach is to replace it with a simple polynomial function, for example a square layer (which simply performs $x \\rightarrow x^2$).\n",
    "\n",
    "We define the model with all the non-linearities removed **approximated**. This model can be re-trained, and it will be ready to be used on encrypted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d4c2478-dabb-487c-99c3-283320a692f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Square(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, t):\n",
    "        return torch.pow(t, 2)\n",
    "\n",
    "lenet_1_approx = torch.load(\"LeNet1_Approx_single_square.pt\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874fb0fa-2122-46d3-8dcf-dc528959da8c",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "From the applicative point of view, we have two options on how we want our Torch model to run on encrypted values:\n",
    "  1. Modify Torch layers code in order to be fully compatible with arrays of Pyfhel ciphertexts/encoded values;\n",
    "  2. Create the code for the general blocks of LeNet-1 (convolutional layer, linear layer, square layer, flatten...)\n",
    "  \n",
    "Let's remember that, in order to be used with the encrypted values, also the weights of the models will have to be **encoded**. This means that each value in the weights of each layer will be encoded in a polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6ff6f65-6b53-486e-a97e-a4952184ff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bfd4c8-79bb-4647-9353-5f76629f1f56",
   "metadata": {},
   "source": [
    "We can now use a function to \"convert\" a PyTorch model to a list of sequential HE-ready-to-be-used layers (`sequential`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "886fc91b-7301-4a31-830a-05b326745c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycrcnn.he.HE import CKKSPyfhel\n",
    "from pycrcnn.model.sequential import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bde28e-7adc-4ede-876a-b5e57b63b0c4",
   "metadata": {},
   "source": [
    "## Encrypted processing\n",
    "\n",
    "Let's list the activities that we will now do:\n",
    "  1. Create a PyCrCNN BFV HE context, specifiying the encryption parameters `m` (polynomial modulus degree) and `p` (plaintext modulus). Let's remember that `q` will be chosen automatically in order to guarantee a 128-bit RSA equivalent security;\n",
    "  2. Convert our Torch approximated model to a list of layers able to work on matrices of encrypted values. The weights will be encoded;\n",
    "  3. Encrypt an image from our testing set;\n",
    "  4. Verify that the final classification result is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d2e537b-1bc0-4dee-8023-4ee75c61a882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images, labels = next(iter(test_loader))\n",
    "\n",
    "sample_image = images[0]\n",
    "sample_label = labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb5ae8c0-20dd-4456-ac49-145b21a17d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd60af67d00>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(sample_image[0], cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d86b2ef-68ad-4eba-865a-b057efcdb8cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a03f8f-997e-4009-94b4-541f7cc56318",
   "metadata": {},
   "source": [
    "We will create two PyCrCNN HE contexts: the one we will use to encrypt the image (`HE_Client`), and the one we will use to process the encrypted image (`HE_Server`). We will need to transfer the public key and the relinearization key in order to allow the server to compute some operations on encrypted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c798006e-31d2-4aa1-a270-58e84c3e8a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The context generation requested 0.95 seconds.\n"
     ]
    }
   ],
   "source": [
    "# La somma dei qi, in bits, ha degli upperbound\n",
    "# legati al valore di n:\n",
    "# 1024 -> 27\n",
    "# 2048 -> 54\n",
    "# 4096 -> 109\n",
    "# 8192 -> 218\n",
    "# 16384 -> 438\n",
    "# 32768 -> 881\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "n_mults = 6\n",
    "m = 8192\n",
    "scale_power = 25\n",
    "\n",
    "encryption_parameters = {\n",
    "    'm': m,                      # For CKKS, n/2 values can be encoded in a single ciphertext\n",
    "    'scale': 2**scale_power,                 # Each multiplication grows the final scale\n",
    "    'qi': [34]+ [scale_power]*n_mults +[34]  # One intermdiate for each multiplication\n",
    "}\n",
    "\n",
    "HE_Client = CKKSPyfhel(**encryption_parameters)\n",
    "HE_Client.generate_keys()\n",
    "HE_Client.generate_relin_keys()\n",
    "\n",
    "public_key = HE_Client.get_public_key()\n",
    "relin_key  = HE_Client.get_relin_key()\n",
    "\n",
    "\n",
    "HE_Server = CKKSPyfhel(**encryption_parameters)\n",
    "HE_Server.load_public_key(public_key)\n",
    "HE_Server.load_relin_key(relin_key)\n",
    "\n",
    "requested_time = round(time.time() - start_time, 2)\n",
    "print(f\"\\nThe context generation requested {requested_time} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a70f52-d87b-489d-b156-2e28333513e3",
   "metadata": {},
   "source": [
    "Now, we create an encoded model starting from the approximated LeNet1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d31b2c1-2fdf-497e-ac6a-2657967a1415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The model encoding requested 4.51 seconds.\n",
      "\n",
      "The image encryption requested 7.25 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "lenet1_approx_encoded = Sequential(HE_Server, lenet_1_approx)\n",
    "requested_time = round(time.time() - start_time, 2)\n",
    "print(f\"\\nThe model encoding requested {requested_time} seconds.\")\n",
    "\n",
    "start_time = time.time()\n",
    "encrypted_image = HE_Client.encrypt_matrix(sample_image.unsqueeze(0).numpy())\n",
    "requested_time = round(time.time() - start_time, 2)\n",
    "print(f\"\\nThe image encryption requested {requested_time} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "161ac5e4-bd96-46a8-b9fb-ab7a78810c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    expected_output = lenet_1_approx(sample_image.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bace3b5-4e38-4698-963b-768d85e707b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The encrypted processing of one image requested 248.92 seconds.\n",
      "\n",
      "The decryption of one result requested 0.02 seconds.\n",
      "\n",
      "The expected result was:\n",
      "tensor([[  5.5308, -11.8797,   3.2904,  -4.3682,  -5.7454,   3.9357,  14.5248,\n",
      "         -10.7057,   3.1197,  -5.0688]])\n",
      "\n",
      "The actual result is: \n",
      "[[  5.9601431  -12.79372053   3.51625164  -4.66144488  -6.13646207\n",
      "    4.17538087  15.61082347 -11.54665622   3.38631164  -5.41632469]]\n",
      "\n",
      "The error is:\n",
      "[[-0.42934228  0.91397409 -0.22581418  0.29328987  0.39104782 -0.23966949\n",
      "  -1.08606895  0.84097728 -0.26662814  0.34751184]]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "encrypted_output = lenet1_approx_encoded(encrypted_image, debug=False)\n",
    "requested_time = round(time.time() - start_time, 2)\n",
    "print(f\"\\nThe encrypted processing of one image requested {requested_time} seconds.\")\n",
    "\n",
    "start_time = time.time()\n",
    "result = HE_Client.decrypt_matrix(encrypted_output)\n",
    "requested_time = round(time.time() - start_time, 2)\n",
    "print(f\"\\nThe decryption of one result requested {requested_time} seconds.\")\n",
    "\n",
    "difference = expected_output.numpy() - result\n",
    "\n",
    "print(f\"\\nThe expected result was:\")\n",
    "print(expected_output)\n",
    "\n",
    "print(f\"\\nThe actual result is: \")\n",
    "print(result)\n",
    "\n",
    "print(f\"\\nThe error is:\")\n",
    "print(difference)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960e28e3-471a-4710-93c8-fc1823a8a259",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCrCNN",
   "language": "python",
   "name": "pycrcnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
