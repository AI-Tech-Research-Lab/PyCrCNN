{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f2be842-80e1-4c95-b6b2-5724545551e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37db485d-fcec-4c20-943e-239a74a70cf4",
   "metadata": {},
   "source": [
    "# Homomorphic Encrypted LeNet-1\n",
    "This notebook will show a very practical example of running the famous LeNet-1 DL model directly on encrypted data.\n",
    "\n",
    "![scheme](HE_processing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b6b3e7-d428-48e4-b9db-3609ceb94b25",
   "metadata": {},
   "source": [
    "## Homomorphic encryption operations\n",
    "First of all, we will look at Pyfhel, a Python library which wraps SEAL, one of the most used frameworks for HE.\n",
    "Pyfhel supports the BFV scheme, so, it is the one that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c74dbc3-b890-4a87-9e8e-0094b9fc17b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Pyfhel obj at 0x7fc46d7345b0, [pk:Y, sk:Y, rtk:-, rlk:-, contx(p=65537, m=4096, base=2, sec=128, dig=64i.32f, batch=False)]>\n"
     ]
    }
   ],
   "source": [
    "from Pyfhel import Pyfhel, PyPtxt, PyCtxt\n",
    "\n",
    "HE = Pyfhel()\n",
    "HE.contextGen(p=65537, m=4096)\n",
    "HE.keyGen()\n",
    "\n",
    "print(HE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9f9b90a-f443-4a6b-9fbb-b1fc91c2565d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected sum: 125.028207448, decrypted sum: 125.02820744784549\n",
      "Expected sub: 129.286137812, decrypted sum: 129.28613781183958\n",
      "Expected mul: -270.7131931708334, decrypted sum: -270.7131931686308\n"
     ]
    }
   ],
   "source": [
    "a = 127.15717263\n",
    "b = -2.128965182\n",
    "ctxt1 = HE.encryptFrac(a)\n",
    "ctxt2 = HE.encryptFrac(b)\n",
    "\n",
    "ctxtSum = ctxt1 + ctxt2\n",
    "ctxtSub = ctxt1 - ctxt2\n",
    "ctxtMul = ctxt1 * ctxt2\n",
    "\n",
    "resSum = HE.decryptFrac(ctxtSum)\n",
    "resSub = HE.decryptFrac(ctxtSub) \n",
    "resMul = HE.decryptFrac(ctxtMul)\n",
    "\n",
    "print(f\"Expected sum: {a+b}, decrypted sum: {resSum}\")\n",
    "print(f\"Expected sub: {a-b}, decrypted sum: {resSub}\")\n",
    "print(f\"Expected mul: {a*b}, decrypted sum: {resMul}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b0293c5-ebf3-4c8b-871f-6f4dd2eb4b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Noise Budget: 82\n",
      "sum Noise Budget: 82\n",
      "prod Noise Budget: 55\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starting Noise Budget: {HE.noiseLevel(ctxt1)}\")\n",
    "\n",
    "print(f\"sum Noise Budget: {HE.noiseLevel(ctxtSum)}\")\n",
    "print(f\"prod Noise Budget: {HE.noiseLevel(ctxtMul)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca445c79-53c2-4507-9438-eb60a5c6302e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ctxt-ptxt prod Noise Budget: 80\n"
     ]
    }
   ],
   "source": [
    "ptxt1 = HE.encodeFrac(-2.128965182)\n",
    "\n",
    "print(f\"ctxt-ptxt prod Noise Budget: {HE.noiseLevel(ctxt1 * ptxt1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1180b17e-7644-4ce8-99a6-c1906758e2eb",
   "metadata": {},
   "source": [
    "## LeNet-1\n",
    "The LeNet-1 is a small CNN developed by LeCun et al. It is composed of 5 layers: a convolutional layer with 4 kernels of size 5x5 and tanh activation, an average pooling layer with kernel of size 2, another convolutional layer with 16 kernels of size 5x5 and tanh activation, another average pooling layer with kernel of size 2, and a fully connected layers with size 192x10. \n",
    "\n",
    "The highest value in the output tensor corresponds to the label LeNet-1 associated to the input image. \n",
    "\n",
    "For this tutorial we will use the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fcf92dc-4929-4adc-b97e-2a1bf946bfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "test_set = torchvision.datasets.MNIST(\n",
    "    root = './data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=50,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025dfbc9-38d0-41a7-a140-9606bfeff53f",
   "metadata": {},
   "source": [
    "## Approximating\n",
    "As we know, there are some operations that cannot be performed homomorphically on encrypted values. Most notably, these operations are division and comparison. It is possible to perform only linear functions.\n",
    "\n",
    "Consequently, in the LeNet-1 scheme we used, we can not use `tanh()`. This is because we cannot apply its non-linearities.\n",
    "\n",
    "\n",
    "One of the most common approach is to replace it with a simple polynomial function, for example a square layer (which simply performs $x \\rightarrow x^2$).\n",
    "\n",
    "We define the model with all the non-linearities removed **approximated**. This model can be re-trained, and it will be ready to be used on encrypted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d4c2478-dabb-487c-99c3-283320a692f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 4, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (1): Square()\n",
       "  (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (3): Conv2d(4, 12, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (4): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (5): Flatten(start_dim=1, end_dim=-1)\n",
       "  (6): Linear(in_features=192, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Square(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, t):\n",
    "        return torch.pow(t, 2)\n",
    "\n",
    "lenet_1_approx = torch.load(\"LeNet1_Approx_single_square.pt\")\n",
    "lenet_1_approx.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874fb0fa-2122-46d3-8dcf-dc528959da8c",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "From the applicative point of view, we have two options on how we want our Torch model to run on encrypted values:\n",
    "  1. Modify Torch layers code in order to be fully compatible with arrays of Pyfhel ciphertexts/encoded values;\n",
    "  2. Create the code for the general blocks of LeNet-1 (convolutional layer, linear layer, square layer, flatten...)\n",
    "  \n",
    "Let's remember that, in order to be used with the encrypted values, also the weights of the models will have to be **encoded**. This means that each value in the weights of each layer will be encoded in a polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6ff6f65-6b53-486e-a97e-a4952184ff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "sys.path.append(module_path)\n",
    "\n",
    "from pycrcnn.crypto.crypto import encrypt_matrix, decrypt_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bfd4c8-79bb-4647-9353-5f76629f1f56",
   "metadata": {},
   "source": [
    "We can now use a function to \"convert\" a PyTorch model to a list of sequential HE-ready-to-be-used layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "886fc91b-7301-4a31-830a-05b326745c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycrcnn.net_builder.encoded_net_builder import build_from_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bde28e-7adc-4ede-876a-b5e57b63b0c4",
   "metadata": {},
   "source": [
    "## Encrypted processing\n",
    "\n",
    "Let's list the activities that we will now do:\n",
    "  1. Create a HE context, specifiying the encryption parameters `m` (polynomial modulus degree) and `p` (plaintext modulus). Let's remember that `q` will be chosen automatically in order to guarantee a 128-bit RSA equivalent security;\n",
    "  2. Convert our Torch approximated model to a list of layers able to work on matrices of encrypted values. The weights will be encoded;\n",
    "  3. Encrypt an image from our testing set;\n",
    "  4. Verify that the final classification result is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d2e537b-1bc0-4dee-8023-4ee75c61a882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images, labels = next(iter(test_loader))\n",
    "\n",
    "sample_image = images[0]\n",
    "sample_label = labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb5ae8c0-20dd-4456-ac49-145b21a17d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc461869c10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(sample_image[0], cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d86b2ef-68ad-4eba-865a-b057efcdb8cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a03f8f-997e-4009-94b4-541f7cc56318",
   "metadata": {},
   "source": [
    "We can define a function which, after receiving $n$ and $p$ tries to encrypt and image and forward it to our approximated model (suitable encoded). This will let us see the homomorphic encryption in function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c798006e-31d2-4aa1-a270-58e84c3e8a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4096\n",
    "p = 953983721\n",
    "\n",
    "HE = Pyfhel()\n",
    "HE.contextGen(p=p, m=n) # what Pyfhel calls m, we call n.\n",
    "HE.keyGen()\n",
    "relinKeySize=3\n",
    "HE.relinKeyGen(bitCount=2, size=relinKeySize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d31b2c1-2fdf-497e-ac6a-2657967a1415",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet1_approx_encoded = build_from_pytorch(HE, lenet_1_approx)\n",
    "encrypted_image = encrypt_matrix(HE, sample_image.unsqueeze(0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "161ac5e4-bd96-46a8-b9fb-ab7a78810c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    expected_output = lenet_1_approx(sample_image.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bace3b5-4e38-4698-963b-768d85e707b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed layer <pycrcnn.convolutional.convolutional_layer.ConvolutionalLayer object at 0x7fc4618510d0>...\n",
      "Passed layer <pycrcnn.functional.square_layer.SquareLayer object at 0x7fc4618511c0>...\n",
      "Passed layer <pycrcnn.functional.average_pool.AveragePoolLayer object at 0x7fc4618511f0>...\n",
      "Passed layer <pycrcnn.convolutional.convolutional_layer.ConvolutionalLayer object at 0x7fc50c5d0df0>...\n",
      "Passed layer <pycrcnn.functional.average_pool.AveragePoolLayer object at 0x7fc50c5d0c40>...\n",
      "Passed layer <pycrcnn.functional.flatten_layer.FlattenLayer object at 0x7fc461851820>...\n",
      "Passed layer <pycrcnn.linear.linear_layer.LinearLayer object at 0x7fc46bf865e0>...\n",
      "\n",
      "The encrypted processing of one image requested 136.67 seconds.\n",
      "\n",
      "The expected result was:\n",
      "tensor([[ -4.1712,  -8.1030,  -0.1250,   2.2501,  -8.6247,  -0.9319, -18.1827,\n",
      "          18.9986,  -1.8027,   5.9015]])\n",
      "\n",
      "The actual result is: \n",
      "[[ -4.16451908  -8.08174058  -0.13303843   2.2423191   -8.61153601\n",
      "   -0.91623444 -18.14454687  18.94905853  -1.79849373   5.87143868]]\n",
      "\n",
      "The error is:\n",
      "[[-0.00667595 -0.02130393  0.00803565  0.00779581 -0.01318551 -0.01562106\n",
      "  -0.03815615  0.04958725 -0.00423564  0.03005773]]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for layer in lenet1_approx_encoded:\n",
    "    encrypted_image = layer(encrypted_image)\n",
    "    print(f\"Passed layer {layer}...\")\n",
    "\n",
    "requested_time = round(time.time() - start_time, 2)\n",
    "\n",
    "result = decrypt_matrix(HE, encrypted_image)\n",
    "difference = expected_output.numpy() - result\n",
    "\n",
    "print(f\"\\nThe encrypted processing of one image requested {requested_time} seconds.\")\n",
    "print(f\"\\nThe expected result was:\")\n",
    "print(expected_output)\n",
    "\n",
    "print(f\"\\nThe actual result is: \")\n",
    "print(result)\n",
    "\n",
    "print(f\"\\nThe error is:\")\n",
    "print(difference)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33d429c-1dfb-4cb4-8e06-59091e0083c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89c04ca-3ec4-4c0a-96f1-5d82801101b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HE_TCNNs",
   "language": "python",
   "name": "he_tcnns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
