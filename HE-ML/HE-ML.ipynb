{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f2be842-80e1-4c95-b6b2-5724545551e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37db485d-fcec-4c20-943e-239a74a70cf4",
   "metadata": {},
   "source": [
    "# Homomorphic Encrypted LeNet-1\n",
    "This notebook will show a very practical example of running the famous LeNet-1 DL model directly on encrypted data.\n",
    "\n",
    "![scheme](HE_processing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b6b3e7-d428-48e4-b9db-3609ceb94b25",
   "metadata": {},
   "source": [
    "## Homomorphic encryption operations\n",
    "First of all, we will look at Pyfhel, a Python library which wraps SEAL, one of the most used frameworks for HE.\n",
    "Pyfhel supports the BFV scheme, so, it is the one that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c74dbc3-b890-4a87-9e8e-0094b9fc17b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Pyfhel obj at 0x7fdef58d36b0, [pk:Y, sk:Y, rtk:-, rlk:-, contx(p=65537, m=4096, base=2, sec=128, dig=64i.32f, batch=False)]>\n"
     ]
    }
   ],
   "source": [
    "from Pyfhel import Pyfhel, PyPtxt, PyCtxt\n",
    "\n",
    "HE = Pyfhel()\n",
    "HE.contextGen(p=65537, m=4096)\n",
    "HE.keyGen()\n",
    "\n",
    "print(HE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9f9b90a-f443-4a6b-9fbb-b1fc91c2565d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected sum: 125.028207448, decrypted sum: 125.02820744784549\n",
      "Expected sub: 129.286137812, decrypted sum: 129.28613781183958\n",
      "Expected mul: -270.7131931708334, decrypted sum: -270.7131931686308\n"
     ]
    }
   ],
   "source": [
    "a = 127.15717263\n",
    "b = -2.128965182\n",
    "ctxt1 = HE.encryptFrac(a)\n",
    "ctxt2 = HE.encryptFrac(b)\n",
    "\n",
    "ctxtSum = ctxt1 + ctxt2\n",
    "ctxtSub = ctxt1 - ctxt2\n",
    "ctxtMul = ctxt1 * ctxt2\n",
    "\n",
    "resSum = HE.decryptFrac(ctxtSum)\n",
    "resSub = HE.decryptFrac(ctxtSub) \n",
    "resMul = HE.decryptFrac(ctxtMul)\n",
    "\n",
    "print(f\"Expected sum: {a+b}, decrypted sum: {resSum}\")\n",
    "print(f\"Expected sub: {a-b}, decrypted sum: {resSub}\")\n",
    "print(f\"Expected mul: {a*b}, decrypted sum: {resMul}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b0293c5-ebf3-4c8b-871f-6f4dd2eb4b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Noise Budget: 82\n",
      "sum Noise Budget: 81\n",
      "prod Noise Budget: 55\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starting Noise Budget: {HE.noiseLevel(ctxt1)}\")\n",
    "\n",
    "print(f\"sum Noise Budget: {HE.noiseLevel(ctxtSum)}\")\n",
    "print(f\"prod Noise Budget: {HE.noiseLevel(ctxtMul)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca445c79-53c2-4507-9438-eb60a5c6302e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ctxt-ptxt prod Noise Budget: 80\n"
     ]
    }
   ],
   "source": [
    "ptxt1 = HE.encodeFrac(-2.128965182)\n",
    "\n",
    "print(f\"ctxt-ptxt prod Noise Budget: {HE.noiseLevel(ctxt1 * ptxt1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1180b17e-7644-4ce8-99a6-c1906758e2eb",
   "metadata": {},
   "source": [
    "## LeNet-1\n",
    "The LeNet-1 is a small CNN developed by LeCun et al. It is composed of 5 layers: a convolutional layer with 4 kernels of size 5x5 and tanh activation, an average pooling layer with kernel of size 2, another convolutional layer with 16 kernels of size 5x5 and tanh activation, another average pooling layer with kernel of size 2, and a fully connected layers with size 192x10. \n",
    "\n",
    "The highest value in the output tensor corresponds to the label LeNet-1 associated to the input image. \n",
    "\n",
    "For this tutorial we will use the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fcf92dc-4929-4adc-b97e-2a1bf946bfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "test_set = torchvision.datasets.MNIST(\n",
    "    root = './data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=50,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025dfbc9-38d0-41a7-a140-9606bfeff53f",
   "metadata": {},
   "source": [
    "## Approximating\n",
    "As we know, there are some operations that cannot be performed homomorphically on encrypted values. Most notably, these operations are division and comparison. It is possible to perform only linear functions.\n",
    "\n",
    "Consequently, in the LeNet-1 scheme we used, we can not use `tanh()`. This is because we cannot apply its non-linearities.\n",
    "\n",
    "\n",
    "One of the most common approach is to replace it with a simple polynomial function, for example a square layer (which simply performs $x \\rightarrow x^2$).\n",
    "\n",
    "We define the model with all the non-linearities removed **approximated**. This model can be re-trained, and it will be ready to be used on encrypted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d4c2478-dabb-487c-99c3-283320a692f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 4, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (1): Square()\n",
       "  (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (3): Conv2d(4, 12, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (4): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (5): Flatten(start_dim=1, end_dim=-1)\n",
       "  (6): Linear(in_features=192, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Square(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, t):\n",
    "        return torch.pow(t, 2)\n",
    "\n",
    "lenet_1_approx = torch.load(\"LeNet1_Approx_single_square.pt\")\n",
    "lenet_1_approx.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874fb0fa-2122-46d3-8dcf-dc528959da8c",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "From the applicative point of view, we have two options on how we want our Torch model to run on encrypted values:\n",
    "  1. Modify Torch layers code in order to be fully compatible with arrays of Pyfhel ciphertexts/encoded values;\n",
    "  2. Create the code for the general blocks of LeNet-1 (convolutional layer, linear layer, square layer, flatten...)\n",
    "  \n",
    "Let's remember that, in order to be used with the encrypted values, also the weights of the models will have to be **encoded**. This means that each value in the weights of each layer will be encoded in a polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6ff6f65-6b53-486e-a97e-a4952184ff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bfd4c8-79bb-4647-9353-5f76629f1f56",
   "metadata": {},
   "source": [
    "We can now use a function to \"convert\" a PyTorch model to a list of sequential HE-ready-to-be-used layers (`sequential`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "886fc91b-7301-4a31-830a-05b326745c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycrcnn.he.HE import BFVPyfhel\n",
    "from pycrcnn.model.sequential import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bde28e-7adc-4ede-876a-b5e57b63b0c4",
   "metadata": {},
   "source": [
    "## Encrypted processing\n",
    "\n",
    "Let's list the activities that we will now do:\n",
    "  1. Create a PyCrCNN BFV HE context, specifiying the encryption parameters `m` (polynomial modulus degree) and `p` (plaintext modulus). Let's remember that `q` will be chosen automatically in order to guarantee a 128-bit RSA equivalent security;\n",
    "  2. Convert our Torch approximated model to a list of layers able to work on matrices of encrypted values. The weights will be encoded;\n",
    "  3. Encrypt an image from our testing set;\n",
    "  4. Verify that the final classification result is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d2e537b-1bc0-4dee-8023-4ee75c61a882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images, labels = next(iter(test_loader))\n",
    "\n",
    "sample_image = images[0]\n",
    "sample_label = labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb5ae8c0-20dd-4456-ac49-145b21a17d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fded6fa2a30>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(sample_image[0], cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d86b2ef-68ad-4eba-865a-b057efcdb8cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a03f8f-997e-4009-94b4-541f7cc56318",
   "metadata": {},
   "source": [
    "We will create two PyCrCNN HE contexts: the one we will use to encrypt the image (`HE_Client`), and the one we will use to process the encrypted image (`HE_Server`). We will need to transfer the public key and the relinearization key in order to allow the server to compute some operations on encrypted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c798006e-31d2-4aa1-a270-58e84c3e8a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4096\n",
    "p = 9539831\n",
    "\n",
    "HE_Client = BFVPyfhel(m=n, p=p)\n",
    "HE_Client.generate_keys()\n",
    "HE_Client.generate_relin_keys()\n",
    "\n",
    "public_key = HE_Client.get_public_key()\n",
    "relin_key  = HE_Client.get_relin_key()\n",
    "\n",
    "\n",
    "HE_Server = BFVPyfhel(m=n, p=p)\n",
    "HE_Server.load_public_key(public_key)\n",
    "HE_Server.load_relin_key(relin_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a70f52-d87b-489d-b156-2e28333513e3",
   "metadata": {},
   "source": [
    "Now, we create an encoded model starting from the approximated LeNet1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d31b2c1-2fdf-497e-ac6a-2657967a1415",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet1_approx_encoded = Sequential(HE_Server, lenet_1_approx)\n",
    "encrypted_image = HE_Client.encrypt_matrix(sample_image.unsqueeze(0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "161ac5e4-bd96-46a8-b9fb-ab7a78810c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    expected_output = lenet_1_approx(sample_image.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bace3b5-4e38-4698-963b-768d85e707b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed layer: <pycrcnn.convolutional.convolutional_layer.ConvolutionalLayer object at 0x7fded719fc40>\n",
      "Noise Budget: Can't get NB without secret key.\n",
      "Passed layer: <pycrcnn.functional.square_layer.SquareLayer object at 0x7fded718aa00>\n",
      "Noise Budget: Can't get NB without secret key.\n",
      "Passed layer: <pycrcnn.functional.average_pool.AveragePoolLayer object at 0x7fdf9469bdc0>\n",
      "Noise Budget: Can't get NB without secret key.\n",
      "Passed layer: <pycrcnn.convolutional.convolutional_layer.ConvolutionalLayer object at 0x7fded719fca0>\n",
      "Noise Budget: Can't get NB without secret key.\n",
      "Passed layer: <pycrcnn.functional.average_pool.AveragePoolLayer object at 0x7fded7447070>\n",
      "Noise Budget: Can't get NB without secret key.\n",
      "Passed layer: <pycrcnn.functional.flatten_layer.FlattenLayer object at 0x7fdef01a5040>\n",
      "Noise Budget: Can't get NB without secret key.\n",
      "Passed layer: <pycrcnn.linear.linear_layer.LinearLayer object at 0x7fdef4dc3b20>\n",
      "Noise Budget: Can't get NB without secret key.\n",
      "\n",
      "The encrypted processing of one image requested 115.16 seconds.\n",
      "\n",
      "The expected result was:\n",
      "tensor([[-11.5013,  -9.9479,  -4.0498,   5.7050,   5.8785,  -0.6192, -22.0101,\n",
      "           2.7510,   4.2004,  16.0299]])\n",
      "\n",
      "The actual result is: \n",
      "[[ -8.79164104  -8.76163449  -4.01551261   5.42078281   4.52327446\n",
      "   -0.63934925 -15.43835083   2.13436893   3.38697838  12.65903401]]\n",
      "\n",
      "The error is:\n",
      "[[-2.70969219 -1.18629203 -0.03433547  0.28420996  1.35521694  0.02015183\n",
      "  -6.57176574  0.61659595  0.81337954  3.37090564]]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "encrypted_output = lenet1_approx_encoded(encrypted_image, debug=True)\n",
    "\n",
    "requested_time = round(time.time() - start_time, 2)\n",
    "\n",
    "result = HE_Client.decrypt_matrix(encrypted_output)\n",
    "difference = expected_output.numpy() - result\n",
    "\n",
    "print(f\"\\nThe encrypted processing of one image requested {requested_time} seconds.\")\n",
    "print(f\"\\nThe expected result was:\")\n",
    "print(expected_output)\n",
    "\n",
    "print(f\"\\nThe actual result is: \")\n",
    "print(result)\n",
    "\n",
    "print(f\"\\nThe error is:\")\n",
    "print(difference)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452f4264-02d5-49c2-9db1-5859d1d94123",
   "metadata": {},
   "source": [
    "In this case we were not able to examine the NB evolution during the computation, because in order to compute the NB we need the secret key. If we use the same PyCrCNN HE context both to encrypt and to process the data, then we will also see the evolution of the NB after each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960e28e3-471a-4710-93c8-fc1823a8a259",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HE_TCNNs",
   "language": "python",
   "name": "he_tcnns"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
